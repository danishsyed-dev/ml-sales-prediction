{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fc3afc44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "if project_root not in sys.path: sys.path.insert(0, project_root)\n",
    "\n",
    "from src._utils import main_logger\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6cf580b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import src.data_handling as data_handling\n",
    "\n",
    "# paths\n",
    "PRODUCTION_MODEL_FOLDER_PATH = 'models/production'\n",
    "DFN_FILE_PATH = os.path.join(PRODUCTION_MODEL_FOLDER_PATH, 'dfn_best.pth')\n",
    "GBM_FILE_PATH =  os.path.join(PRODUCTION_MODEL_FOLDER_PATH, 'gbm_best.pth')\n",
    "EN_FILE_PATH = os.path.join(PRODUCTION_MODEL_FOLDER_PATH, 'en_best.pth')\n",
    "\n",
    "PREPROCESSOR_PATH = 'preprocessors/column_transformer.pkl'\n",
    "\n",
    "file_name = 'online_retail.csv'\n",
    "file_path = os.path.join(project_root, 'data', 'raw', file_name)\n",
    "df = pd.read_csv(file_path)\n",
    "df = data_handling.scripts.sanitize_column_names(df=df)\n",
    "df = data_handling.scripts.structure_missing_values(df=df)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9d1c3d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### stockcode\n",
    "df['stockcode'] = df['stockcode'].replace('m', 'M')\n",
    "stockcodes_to_drop = ['B', 'CRUK', 'C2']\n",
    "df = df[~df['stockcode'].isin(stockcodes_to_drop)]\n",
    "\n",
    "#### remove unnecessary features\n",
    "if 'description' in df.columns.tolist(): df = df.drop(columns='description')\n",
    "\n",
    "\n",
    "#### adds quantity momentum features\n",
    "df['invoicedate'] = pd.to_datetime(df['invoicedate'], errors='coerce')\n",
    "df['year'] = df['invoicedate'].dt.year\n",
    "df['year_month'] = df['invoicedate'].dt.to_period('M')\n",
    "df['month_name'] = df['invoicedate'].dt.strftime('%b')\n",
    "df['day_of_week'] = df['invoicedate'].dt.strftime('%a')\n",
    "# df['sales'] = df['quantity'] * df['unitprice']\n",
    "\n",
    "### last month\n",
    "_temp_df_prod_month_agg = df.copy().groupby(['stockcode', 'year_month']).agg(\n",
    "    prod_total_monthly_quantity=('quantity', 'sum'),\n",
    "    prod_ave_monthly_price=('unitprice', 'mean')\n",
    ").reset_index().sort_values(by=['stockcode', 'year_month'])\n",
    "_temp_df_prod_month_agg['product_avg_quantity_last_month'] = _temp_df_prod_month_agg.groupby('stockcode')['prod_total_monthly_quantity'].shift(1)\n",
    "_temp_df_prod_last_month_agg = _temp_df_prod_month_agg.groupby('stockcode')['product_avg_quantity_last_month'].mean().reset_index()\n",
    "df_fin = pd.merge(df, _temp_df_prod_last_month_agg, on='stockcode', how='left')\n",
    "df_fin['product_avg_quantity_last_month'] = df_fin['product_avg_quantity_last_month'].fillna(value=0)\n",
    "\n",
    "\n",
    "#### add unitprice related features\n",
    "### vs max\n",
    "_temp_df_max_price = df.groupby('stockcode')['unitprice'].max().reset_index()\n",
    "_temp_df_max_price.rename(columns={'unitprice': 'product_max_price_all_time'}, inplace=True)\n",
    "df_fin = pd.merge(left=df_fin, right=_temp_df_max_price, on='stockcode', how='left')\n",
    "\n",
    "df_fin['unitprice_vs_max'] = df_fin['unitprice'] / df_fin['product_max_price_all_time']\n",
    "\n",
    "### vs ave\n",
    "df_fin['unitprice_to_avg'] = df_fin['unitprice'] / df_fin['product_avg_quantity_last_month']\n",
    "df_fin.loc[df_fin['product_avg_quantity_last_month'] == 0, 'unitprice_to_avg'] = 0\n",
    "\n",
    "### squared\n",
    "df_fin['unitprice_squared'] = df_fin['unitprice'] ** 2\n",
    "\n",
    "### log\n",
    "df_fin['unitprice_log'] = np.log1p(df_fin['unitprice'])\n",
    "\n",
    "\n",
    "####  add customer related features\n",
    "df_fin['is_registered'] = np.where(df_fin['customerid'].isna(), 0, 1)\n",
    "df_fin['customerid'] = df_fin['customerid'].fillna('unknown').astype('str')\n",
    "\n",
    "# _df_cus = df_fin.groupby(['customerid', 'stockcode', 'year_month']).agg(\n",
    "#     monthly_quantity=('quantity', 'sum'),\n",
    "#     monthly_unique_invoices=('invoiceno', 'nunique'),\n",
    "#     monthly_last_purchase_date=('invoicedate', 'max')\n",
    "# ).reset_index()\n",
    "# _df_cus['pfin_last_purchase_date_stockcode'] = _df_cus.groupby(['customerid', 'stockcode'])['monthly_last_purchase_date'].shift(1)\n",
    "# _df_cus['invoice_timestamp_end'] = _df_cus['year_month'].dt.end_time\n",
    "# _df_cus['customer_recency_days_by_stockcode'] = (\n",
    "#     _df_cus['invoice_timestamp_end'] - _df_cus['pfin_last_purchase_date_stockcode']\n",
    "# ).dt.days\n",
    "# df_fin = df_fin.merge(\n",
    "#     _df_cus[['customerid', 'stockcode', 'year_month', 'customer_recency_days_by_stockcode']],\n",
    "#     on=['customerid', 'stockcode', 'year_month'],\n",
    "#     how='left'\n",
    "# )\n",
    "# # imputation\n",
    "# max_recency = df_fin['customer_recency_days_by_stockcode'].max()\n",
    "# df_fin['customer_recency_days_by_stockcode'] = df_fin['customer_recency_days_by_stockcode'].fillna(value=max_recency + 30)\n",
    "# df_fin['customer_recency_days_by_stockcode'] = df_fin['customer_recency_days_by_stockcode'].fillna(365)\n",
    "\n",
    "\n",
    "# # spent \n",
    "# if not _df_cus['customerid'].isna().all():\n",
    "#     _df_cus_prod_agg = _df_cus.groupby(['customerid', 'stockcode', 'year_month']).agg(\n",
    "#         monthly_quantity=('monthly_quantity', 'sum'),\n",
    "#         monthly_unique_invoices=('monthly_unique_invoices', 'sum')\n",
    "#     ).reset_index()\n",
    "\n",
    "#     # Sort by customer, stockcode, and date to ensure rolling window is chronological\n",
    "#     _df_cus_prod_agg = _df_cus_prod_agg.sort_values(by=['customerid', 'stockcode', 'year_month'])\n",
    "\n",
    "#     # Compute customer_total_spend_ltm and customer_freq_ltm\n",
    "#     _df_cus_prod_agg['customer_total_spend_ltm_by_stockcode'] = _df_cus_prod_agg.groupby(['customerid', 'stockcode'])['monthly_quantity'].rolling(window=3, closed='left').sum().reset_index(level=[0, 1], drop=True)\n",
    "#     _df_cus_prod_agg['customer_freq_ltm_by_stockcode'] = _df_cus_prod_agg.groupby(['customerid', 'stockcode'])['monthly_unique_invoices'].rolling(window=3, closed='left').sum().reset_index(level=[0, 1], drop=True)\n",
    "\n",
    "#     # Merge the new features back into the main DataFrame\n",
    "#     df_fin = pd.merge(\n",
    "#         df_fin,\n",
    "#         _df_cus_prod_agg[['customerid', 'stockcode', 'year_month', 'customer_total_spend_ltm_by_stockcode', 'customer_freq_ltm_by_stockcode']],\n",
    "#         on=['customerid', 'stockcode', 'year_month'],\n",
    "#         how='left'\n",
    "#     )\n",
    "\n",
    "#     # Fill NaN values with 0\n",
    "#     df_fin['customer_total_spend_ltm_by_stockcode'] = df_fin['customer_total_spend_ltm_by_stockcode'].fillna(0)\n",
    "#     df_fin['customer_freq_ltm_by_stockcode'] = df_fin['customer_freq_ltm_by_stockcode'].fillna(0)\n",
    "\n",
    "# else:\n",
    "#     df_fin['customer_freq_ltm_by_stockcode'] = 0\n",
    "#     df_fin['customer_total_spend_ltm_by_stockcode'] = 0\n",
    "\n",
    "\n",
    "# drop unnecessary data\n",
    "stockcodes_to_drop = ['D', 'S']\n",
    "df_fin = df_fin[~df_fin['stockcode'].isin(stockcodes_to_drop)]\n",
    "df_fin = df_fin.drop(columns=['month_name'], axis='columns')\n",
    "\n",
    "\n",
    "# quantity (drop negative vals)\n",
    "df_fin['quantity'] = pd.to_numeric(df_fin['quantity'], errors='coerce')\n",
    "df_fin['quantity'] = df_fin['quantity'].fillna(0)\n",
    "df_fin = df_fin[df_fin['quantity'] > 0]\n",
    "\n",
    "\n",
    "# dtype transformation\n",
    "df_fin['year_month'] = df_fin['year_month'].dt.month\n",
    "df_fin['invoicedate'] = df_fin['invoicedate'].astype(int) / 10 ** 9\n",
    "\n",
    "\n",
    "# imputation\n",
    "df_fin['customerid'] = df_fin['customerid'].fillna(value='unknown')\n",
    "df_fin['stockcode'] = df_fin['stockcode'].fillna(value='unknown')\n",
    "df_fin['invoiceno'] = df_fin['invoiceno'].fillna(value='unknown')\n",
    "# df_fin['quantity'] = df_fin['quantity'].fillna(value=0)\n",
    "\n",
    "# imputation (values referred to stockcode)\n",
    "df_imputed = df_fin.copy().sort_values(by='stockcode').reset_index(drop=True)\n",
    "df_stockcode = df_imputed.groupby('stockcode', as_index=False).agg(\n",
    "    imputed_country=('country', lambda x: x.mode().iloc[0] if not x.mode().empty else 'unknown'),\n",
    "    imputed_unitprice=('unitprice', 'median')\n",
    ")\n",
    "df_fin = pd.merge(df_fin, df_stockcode, on='stockcode', how='left')\n",
    "df_fin['country'] = df_fin['country'].fillna(df_fin['imputed_country'])\n",
    "\n",
    "global_median = df_fin['unitprice'].median()\n",
    "df_fin['unitprice'] = df_fin['unitprice'].fillna(df_fin['imputed_unitprice'])\n",
    "df_fin['unitprice'] = df_fin['unitprice'].fillna(global_median)\n",
    "df_fin = df_fin.drop(columns=['imputed_country', 'imputed_unitprice'])\n",
    "\n",
    "# df_fin['is_return'] = (df_fin['sales'] < 0).astype(int)\n",
    "# drop negative sales (return etc)\n",
    "# df_fin_log = df_fin.copy()\n",
    "# df_fin_log = df_fin_log[df_fin_log['sales'] > 0]\n",
    "# df_fin_log['sales'] = np.where(df_fin_log['is_return'] == 1, 0, df_fin_log['sales'])\n",
    "\n",
    "# transform sales to logged values\n",
    "alpha = 1e-10\n",
    "df_fin['quantity'] = np.log1p(df_fin['quantity'] + alpha)\n",
    "\n",
    "main_logger.info(df_fin.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fa0b6493",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_fin.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "25f51e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import src.model.torch_model as t\n",
    "\n",
    "\n",
    "# classify num and cat columns\n",
    "target_col = 'quantity'\n",
    "\n",
    "num_cols, cat_cols = data_handling.scripts.categorize_num_cat_cols(df=df, target_col=target_col)\n",
    "if cat_cols: \n",
    "    for col in cat_cols: df[col] = df[col].astype('string')\n",
    "\n",
    "\n",
    "# # creates train, val, test datasets\n",
    "# y = df[target_col]\n",
    "# X = df.copy().drop(target_col, axis='columns')\n",
    "\n",
    "# test_size, random_state = 50000, 42\n",
    "# X_tv, X_test, y_tv, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X_tv, y_tv, test_size=test_size, random_state=random_state)\n",
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.impute import SimpleImputer, KNNImputer\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.compose import ColumnTransformer\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from category_encoders import BinaryEncoder\n",
    "\n",
    "\n",
    "# num_transformer = Pipeline(steps=[\n",
    "#     # ('imputer', KNNImputer()), \n",
    "#     ('imputer', SimpleImputer(strategy='median')),\n",
    "#     ('scaler', StandardScaler())\n",
    "#     ])\n",
    "# cat_transformer = Pipeline(steps=[\n",
    "#     # ('imputer', SimpleImputer(strategy='most_frequent', fill_value='unknown')),\n",
    "#     ('encoder', BinaryEncoder(cols=cat_cols))\n",
    "# ])\n",
    "# preprocessor = ColumnTransformer(\n",
    "#     transformers=[\n",
    "#         ('num', num_transformer, num_cols),\n",
    "#         ('cat', cat_transformer, cat_cols)\n",
    "#     ],\n",
    "#     remainder='passthrough'\n",
    "# )\n",
    "\n",
    "# X_train_processed = preprocessor.fit_transform(X_train)\n",
    "# X_val_processed = preprocessor.transform(X_val)\n",
    "# X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "# main_logger.info(f'transformed input datasets: X_train: {X_train_processed.shape}, X_val: {X_val_processed.shape}, X_test: {X_test_processed.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bd3188f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import joblib\n",
    "\n",
    "stockcode = '85123A'\n",
    "random_state = 42\n",
    "PREPROCESSOR_PATH = os.path.join(project_root, 'preprocessors/column_transformer.pkl')\n",
    "preprocessor = joblib.load(PREPROCESSOR_PATH)\n",
    "\n",
    "df_stockcode = df[df['stockcode'] == stockcode]\n",
    "X_stockcode = df_stockcode.copy().drop(columns=target_col)\n",
    "y_stockcode = df_stockcode.copy()[target_col]\n",
    "\n",
    "X_tv, X_test_stockcode, y_tv, y_test_stockcode = train_test_split(X_stockcode, y_stockcode, test_size=500, random_state=random_state)\n",
    "X_train_stockcode, X_val_stockcode, y_train_stockcode, y_val_stockcode = train_test_split(X_tv, y_tv, test_size=500, random_state=random_state)\n",
    "\n",
    "X_train_stockcode = preprocessor.transform(X_train_stockcode)\n",
    "X_val_stockcode = preprocessor.transform(X_val_stockcode)\n",
    "\n",
    "batch_size = 32\n",
    "train_data_loader_stockcode = t.scripts.create_torch_data_loader(X=X_train_stockcode, y=y_train_stockcode, batch_size=batch_size)\n",
    "val_data_loader_stockcode = t.scripts.create_torch_data_loader(X=X_val_stockcode, y=y_val_stockcode, batch_size=batch_size)\n",
    "\n",
    "file_path = os.path.join(project_root, 'models/production/dfn_best.pth')\n",
    "model = t.scripts.load_model(input_dim=X_train_stockcode.shape[1], file_path=file_path)\n",
    "\n",
    "# retrain the best model\n",
    "model, _ = t.scripts.train_model(\n",
    "    model=model,\n",
    "    optimizer=torch.optim.Adam(model.parameters(), lr=0.001),\n",
    "    criterion=torch.nn.MSELoss(),\n",
    "    num_epochs=1000,\n",
    "    min_delta=0.00001,\n",
    "    patience=10,\n",
    "    train_data_loader=train_data_loader_stockcode,\n",
    "    val_data_loader=val_data_loader_stockcode,\n",
    "    device_type='cpu'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "10100fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "DFN_FILE_PATH_OVERALL_BEST = os.path.join(PRODUCTION_MODEL_FOLDER_PATH, 'dfn_best.pth')\n",
    "DFN_FILE_PATH_STOCKCODE = os.path.join(PRODUCTION_MODEL_FOLDER_PATH, f'dfn_best_{stockcode}.pth')\n",
    "PREPROCESSOR_PATH = os.path.join(project_root, 'preprocessors/column_transformer.pkl')\n",
    "\n",
    "# preprocessor (trained on data)\n",
    "preprocessor = joblib.load(PREPROCESSOR_PATH)\n",
    "\n",
    "\n",
    "df_stockcode = df[df['stockcode'] == stockcode]\n",
    "\n",
    "target_col = 'quantity'\n",
    "X_stockcode = df_stockcode.copy().drop(columns=target_col)\n",
    "y_stockcode = df_stockcode.copy()[target_col]\n",
    "\n",
    "test_size, random_state = int(min(len(X_stockcode) * 0.3, 500)), 42  # type: ignore\n",
    "X_tv, X_test_stockcode, y_tv, y_test_stockcode = train_test_split(X_stockcode, y_stockcode, test_size=test_size, random_state=random_state)\n",
    "X_train_stockcode, X_val_stockcode, y_train_stockcode, y_val_stockcode = train_test_split(X_tv, y_tv, test_size=test_size, random_state=random_state)\n",
    "\n",
    "X_train_stockcode = preprocessor.transform(X_train_stockcode)\n",
    "X_val_stockcode = preprocessor.transform(X_val_stockcode)\n",
    "\n",
    "batch_size = 32\n",
    "train_data_loader_stockcode = t.scripts.create_torch_data_loader(X=X_train_stockcode, y=y_train_stockcode, batch_size=batch_size)\n",
    "val_data_loader_stockcode = t.scripts.create_torch_data_loader(X=X_val_stockcode, y=y_val_stockcode, batch_size=batch_size)\n",
    "\n",
    "file_path = os.path.join(project_root, 'models/production/dfn_best.pth')\n",
    "model = t.scripts.load_model(input_dim=X_train_stockcode.shape[1], file_path=file_path)\n",
    "\n",
    "# retrain the best model\n",
    "model, _ = t.scripts.train_model(\n",
    "    model=model,\n",
    "    optimizer=torch.optim.Adam(model.parameters(), lr=0.001),\n",
    "    criterion=torch.nn.MSELoss(),\n",
    "    num_epochs=1000,\n",
    "    min_delta=0.00001,\n",
    "    patience=10,\n",
    "    train_data_loader=train_data_loader_stockcode,\n",
    "    val_data_loader=val_data_loader_stockcode,\n",
    "    device_type='cpu'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9768a9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "from src._utils import main_logger, s3_load\n",
    "import src.model.torch_model as t\n",
    "\n",
    "X_TEST_PATH = os.environ.get('X_TEST','data/x_test_df.parquet')\n",
    "\n",
    "stockcode = '85123A'\n",
    "\n",
    "# df = df_fin.copy()\n",
    "# df_stockcode = df[df['stockcode'] == stockcode]\n",
    "\n",
    "# file_path = os.path.join(project_root, 'models', 'production', f'dfn_best_{stockcode}.pth')\n",
    "# model = t.scripts.load_model(input_dim=65, file_path=file_path)\n",
    "\n",
    "x_test_io = s3_load(file_path=X_TEST_PATH)\n",
    "X_test = pd.read_parquet(x_test_io) if x_test_io is not None else None\n",
    "\n",
    "\n",
    "\n",
    "if X_test is not None:\n",
    "    min_price = 2\n",
    "    max_price = 100\n",
    "    NUM_PRICE_BINS = 100\n",
    "    price_range = np.linspace(min_price, max_price, num=NUM_PRICE_BINS)\n",
    "    price_range_df = pd.DataFrame({ 'unitprice': price_range })\n",
    "\n",
    "    test_sample = X_test.sample(n=5000, random_state=42) \n",
    "    test_sample_merged = test_sample.merge(price_range_df, how='cross')\n",
    "    test_sample_merged.drop('unitprice_x', axis=1, inplace=True)\n",
    "    test_sample_merged.rename(columns={'unitprice_y': 'unitprice'}, inplace=True)\n",
    "\n",
    "    X = preprocessor.transform(test_sample_merged) if preprocessor else test_sample_merged\n",
    "\n",
    "    model.eval()\n",
    "    input_tensor = torch.tensor(X, dtype=torch.float32, device=torch.device('cpu'))\n",
    "    epsilon = 0\n",
    "    with torch.inference_mode():\n",
    "        y_pred = model(input_tensor)\n",
    "        y_pred = y_pred.cpu().numpy().flatten()\n",
    "        y_pred_actual = np.exp(y_pred + epsilon)\n",
    "        main_logger.info(f\"primary model's prediction for stockcode {stockcode} - actual quantity {y_pred_actual[0: 10]}\")\n",
    "\n",
    "\n",
    "    df_ = test_sample_merged.copy()\n",
    "    df_['quantity'] = np.floor(y_pred_actual * 10)\n",
    "    df_['sales'] = df_['quantity'] * df_['unitprice']\n",
    "    df_ = df_.sort_values(by='unitprice')\n",
    "\n",
    "    df_results = df_.groupby('unitprice').agg(\n",
    "        quantity=('quantity', 'mean'),\n",
    "        quantity_min=('quantity', 'min'),\n",
    "        quantity_max=('quantity', 'max'),\n",
    "        sales=('sales', 'mean'),\n",
    "    ).reset_index()\n",
    "\n",
    "    optimal_row = df_results.loc[df_results['sales'].idxmax()]\n",
    "    optimal_price = optimal_row['unitprice']\n",
    "    optimal_quantity = optimal_row['quantity']\n",
    "    best_sales = optimal_row['sales']\n",
    "\n",
    "    all_outputs = []\n",
    "    for _, row in df_results.iterrows():\n",
    "        current_output = {\n",
    "            \"stockcode\": stockcode,\n",
    "            \"unit_price\": float(row['unitprice']),\n",
    "            'quantity': int(row['quantity']),\n",
    "            'quantity_min': int(row['quantity_min']),\n",
    "            'quantity_max': int(row['quantity_max']),\n",
    "            \"predicted_sales\": float(row['sales']),\n",
    "            \"optimal_unit_price\": float(optimal_price), # type: ignore\n",
    "            \"max_predicted_sales\": float(best_sales), # type: ignore\n",
    "        }\n",
    "        all_outputs.append(current_output)\n",
    "\n",
    "    main_logger.info(f'optimal price: $ {optimal_price:,.2f}, quantity: {optimal_quantity:,}, maximum sales: $ {best_sales:,.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff02c56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# demand curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "plt.plot(df_results['quantity'], df_results['unitprice'], color='darkblue', label='demand curve')\n",
    "\n",
    "# Highlight the optimal price point on the graph\n",
    "plt.hlines(y=optimal_price, xmin=min(df_results['quantity']), xmax=max(df_results['quantity']), color='darkred', zorder=5, label='Optimal Price Point')\n",
    "\n",
    "# Add labels and a title for clarity\n",
    "plt.xlabel('quantity', fontsize=12)\n",
    "plt.ylabel('unitprice', fontsize=12)\n",
    "plt.title('Demand Curve', fontsize=14)\n",
    "plt.legend()\n",
    "plt.yticks(df_results['unitprice']) # Ensure all price points are shown on the y-axis\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "facd1cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sales\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "# x_plt = df_['unitprice'][df_.index % 100 == 0]\n",
    "# y_plt = df_['sales'][df_.index % 100 == 0]\n",
    "\n",
    "plt.plot(df_results['unitprice'], df_results['sales'], color='darkblue', label='Predicted Sales')\n",
    "\n",
    "# Highlight the optimal price point on the graph\n",
    "plt.scatter(optimal_price, best_sales, color='darkred', s=100, zorder=5, label='Optimal Price Point')\n",
    "\n",
    "# Add labels and a title for clarity\n",
    "plt.xlabel('Unit Price', fontsize=12)\n",
    "plt.ylabel('Predicted Sales', fontsize=12)\n",
    "plt.title('Predicted Sales vs. Unit Price', fontsize=14)\n",
    "plt.legend()\n",
    "\n",
    "plt.xticks(df_results['unitprice']) # Ensure all price points are shown on the x-axis\n",
    "\n",
    "# Annotate the optimal point with its values\n",
    "plt.annotate(\n",
    "    f'Optimal Price: ${optimal_price:,.2f}\\nMax Sales: ${best_sales:,.2f}',\n",
    "    xy=(optimal_price, best_sales),\n",
    "    xytext=(optimal_price + 2, best_sales + 10),\n",
    "    arrowprops=dict(facecolor='black', shrink=0.05),\n",
    "    fontsize=10\n",
    ")\n",
    "# plt.savefig('my_plot.png')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_tf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}